---
title: "CLW_R_Learning：Logistic Regression"
author: "Jason Wang"
date: "Tuesday, April 21, 2015"
output:
  pdf_document:
    fig_height: 5
    fig_width: 5
    latex_engine: xelatex
fontsize: 12pt
mainfont: Calibri
---
# 1. Little Introduction to Generalized Linear Models(GLM)

There are three components in the GLM:

* Random Component $-$ the response variable $Y$ and an associated probability distribution.

* Systematic Component $-$ the explanatory variables $x_{1},...,x_{p}$

* Link Function $-$ the functional relationship between $x_{1},...,x_{p}$ and $E(Y)$


 $Y$ | Link function
 ------------- | -------------
$N(\mu,\sigma^2)$ | $E(Y)$
$Poisson(\lambda)$ | $log(E(Y))$
$Binomial(n, p)$ | $log \dfrac{E(Y)}{n-E(Y)}$


# 2. Logistic Regression

First, we introduce the logstic function by a illustration.

\begin{center}
$p(x) = $ $\dfrac{e^{\alpha + \beta x}}{1+e^{\alpha + \beta x}}$
\end{center}

```{r}
logistic <- function(x, alpha, beta){
    exp(alpha + beta*x)/(1 + exp(alpha + beta*x))
}
```

```{r, fig.align='center',fig.width=6}
x <- seq(-10, 10, by=0.1)
plot(x, logistic(x, 0, -1), type="l",
     main="Logistic Function", col="darkorange",
     xlab="x", ylab="probability", lwd=4)
lines(x, logistic(x, 0, 1), type="l",
      col="dodgerblue", lwd=4)
text(8, 0.9, expression(beta), cex=1.5); text(-8, 0.9, expression(beta), cex=1.5)
text(9, 0.9, "  > 0", cex=1.5); text(-7, 0.9, "  < 0", cex=1.5)

lines(x, logistic(x, 0, 0), type="l", lwd=4)
text(1.5, 0.55, expression(beta), cex=1.5)
text(2.5, 0.55, "  = 0", cex=1.5)
```
If $\beta$ is bigger than 0, then the probability will increase as x goes up; on the other hand, if $\beta$ is less than 0, the probability will decrease as x goes up. Finally, when $\beta$ is equal to 1, the probability will be 0.5 given $\alpha$ is 0.

With the above illustration, it gives us a hint or implication that whether we can utilize this kind of function to fit a model on the binary response data. Hence, here come to the logistic regression.

\begin{center}
$E(y|X) = 1 \times P(y=1|X) + 0 \times P(y=0|X) = P(y=1|X)$
\end{center}

\newpage

We model $E(y|X)$ using the logistic function that gives outputs between 0 and 1 for all values of $X$.

\begin{center}
$p = E(y|X) = \dfrac{e^{\alpha + \beta_{1}x_{1} + \beta_{2}x_{2} + ... + \beta_{p}x_{p}}}{1+e^{\alpha + \beta_{1}x_{1} + \beta_{2}x_{2} + ... + \beta_{p}x_{p}}}$
\end{center}

With some manipulation, we can get

\begin{center}
$\dfrac{p}{1-p} = e^{\alpha + \beta_{1}x_{1} + \beta_{2}x_{2} + ... + \beta_{p}x_{p}}$
\end{center}

$\frac{p}{1-p}$ is called the odds and take on any value from 0 to $\infty$. If the odds is high, the probability is high, and vice versa.

Take log for both side

\begin{center}
$log(\dfrac{p}{1-p}) = \alpha + \beta_{1}x_{1} + \beta_{2}x_{2} + ... + \beta_{p}x_{p}$
\end{center}

From the left-hand side, we get a function called *logit* function. $logit(p) = log(\frac{p}{1-p})$. The logit function is exactly the **link function** between $E(Y)$ and $x_{1},...,x_{p}$.

* Interpretation

The interpretation of the coefficients $x_{i}$ will be totally different from the linear regression. In logistic regression, the interpretation is that given other variables fix constant, increasing $x_{i}$ by one unit change the log-odds of $E(Y)$ or $p$ by $\beta_{i}$ unit or, equivalently, multiplies the odds of $p$ by $e^{\beta_{i}}$. Most importantly, The amount that $p$ changes due to a one-unit change in
$x_{i}$ will depend on the current value of $x_{i}$.

* Estimation of Coefficients

In logistic regression, we use *maximum likelihood estimation* to estimate coefficients. Recall that our $Y$ is from the binomial distribution (can be view as bernoulli trials when solving the likelihood function), assuming n data, we can use the *likelihood function*:

\begin{center}
$l(\alpha, \beta_{1}, \beta_{2},..., \beta_{p}) = \prod_{i=1}^{n} p(y_{i}|x_{i})$　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　\\
$= \prod_{i=1}^{n} [\dfrac{e^{\alpha + \beta_{1}x_{1i} + \beta_{2}x_{2i} + ... + \beta_{p}x_{pi}}}{1+e^{\alpha + \beta_{1}x_{1i} + \beta_{2}x_{2i} + ... + \beta_{p}x_{pi}}}]^{y_{i}} [\dfrac{1}{1+e^{\alpha + \beta_{1}x_{1i} + \beta_{2}x_{2i} + ... + \beta_{p}x_{pi}}}]^{1-y_{i}}$
\end{center}

and our goal is to find out the coefficients $\alpha, \beta_{1}, \beta_{2},...,\beta_{p}$ which maximize the likelihood function. Normally, we will transformation (log) the likelihood to *deviance*:

\begin{center}
$D = -2[\sum_{i=1}^{n} y_{i}log(p_{i}) + \sum_{i=1}^{n} (1-y_{i})log(1-p{i})]$\\
where　$p_{i} = \dfrac{e^{\alpha + \beta_{1}x_{1i} + \beta_{2}x_{2i} + ... + \beta_{p}x_{pi}}}{1+e^{\alpha + \beta_{1}x_{1i} + \beta_{2}x_{2i} + ... + \beta_{p}x_{pi}}}$
\end{center}

After the transformation, the goal change to find the parameters that minimize the deviance. Fortunately, we can use R to solve the coefficients effectively. (Note: In R, its output is -2log($l$), i.e. Deviance, while in SAS, the output is log($l$), i.e. Log likelihood function)

\newpage

```{r}
#Deviance illustration
null_deviance_bin <- function(y){
    p <- mean(y)
    D <- -2*(sum(y*log(p) + (1 - y)*log(1 - p)))
    D
}
```

```{r}
y <- rbinom(100, 1, 0.8)
x <- rnorm(100)
model <- glm(y ~ x, family=binomial)
#Without variable
summary(model)$null.deviance
```

```{r}
null_deviance_bin(y)
```

```{r}
#With variable
summary(model)$deviance
```

```{r}
beta <- coef(model)
model_deviance <- function(y, x, beta){
    e <- exp(cbind(rep(1, length(x)), x)%*%beta)
    p <- e/(1 + e)
    D <- -2*(sum(y*log(p) + (1 - y)*log(1 - p)))
    D
}
model_deviance(y, x, beta)
```

The idea of deivance is really similar to sum of square in regression. Summary:
\begin{center}
Regression model → analysis of variance (compare "sum of square")\\
Generalized linear model → analysis of deviance (compare "deviance")
\end{center}

\newpage

* Three Main Types of Statistical Tests for GLMs

1. Wald test
\begin{center}
the test statsitic　$Z = \dfrac{\hat \beta}{ASE}$　$\xrightarrow[ ]{D}$　$N(0, 1)$
\end{center}

The summary function to glm will automatically provide us the Wald test.

```{r}
coef(summary(model))
```


2. Likelihood ratio test
\begin{center}
the likelihood-ratio statistic$= -2log(\dfrac{l_{o}}{l_{1}}) = -2(log l_{0} - log l_{1}) = -2(L_{0} - L_{1})$　$\xrightarrow[ ]{D}$　$\chi^2_{1}$
\end{center}

```{r, message=FALSE}
library(lmtest)
#Likelihood ratio test
lrtest(model)
```

The log likelihood function here is equal to the deviance divided by 2. Also we can use the anova function to do likelihood ratio test.

```{r}
anova(model, test="LRT")
```

3. Score test
\begin{center}
the efficient score statistics　$\xrightarrow[ ]{D}$　$\chi^2_{1}$
\end{center}

We can use the anova function to do likelihood ratio test.
```{r}
anova(model, test="Rao")
```

In generally, the three test would give you the same conclusion for significance. If it is not, the suggestion is to collect more data. However, if you cannot get more data, then take the likelihood ratio test because LR test is more conservative.

# 3. Multinomial Logistic Regression

Also, we can generalize the method to multinomial situation. Assume that we have g possible outcomes with probabilities $P[y=k]=p_{k}, k=1, 2,..., g.$

\begin{center}
$p_{1}=p[y=1]=$ $\dfrac{1}{1+\sum_{i=2}^{g}e^{\alpha_{i}+x\beta_{i}}}$\\
$p_{k}=p[y=k]=$ $\dfrac{e^{\alpha_{k}+x\beta_{k}}}{1+\sum_{i=2}^{g}e^{\alpha_{i}+x\beta_{i}}}$ for $k=2,..., g$
\end{center}

The sum of $p_{1}, p_{2},..., p_{g}$ is equal to 1. The category 1 here is our standard category while any other group could be use instead.

The log odds interpretation of the logistic regression model still applies, as
\begin{center}
$log(\dfrac{p_{k}}{p_{1}})=\alpha_{k}+x\beta_{k}$ for $k=2, ...g$.
\end{center}

However, the interpretation become more complicated. Changing the the explanatory variable $x_{i}$ by on unit changes the odds of getting an outcome from group $k$ relative to getting an outcome from gropu 1. (Note the log odds now become $p_{k}$ v.s. $p_{1}$) For instance, if $\beta_{k}$ = 1.5, per unit change in variable $x_{i}$ increase the odds by mutiplying by $e^{1.5}$ = `r round(exp(1.5), 2)`. Similarly, per unit change in x changes the odds of getting an outcome from group $k$ relative to getting an outcome from group $r$ by factor $e^{\beta_{k}-\beta_{r}}$ because

\begin{center}
$\dfrac{p_{k}}{p_{r}}=$$\dfrac{e^{\alpha_{k}+x\beta_{k}}}{e^{\alpha_{r}+x\beta_{r}}}=$$e^{\alpha{k}-\alpha{r}+x(\beta_{k}-\beta{r})}$\\
$log(\dfrac{p_{k}}{p_{1}})=\alpha_{k}-\alpha_{r}+x(\beta_{k}-\beta{r})$
\end{center}

The interpreation is so complex, which may be the reason why the textbook skips this session. We generate a fictional data to run the multinomail logistic regression.
```{r, message=FALSE}
#5 categories
y <- gl(5, 20)
#normal which increases the mean
x <- rnorm(100, rep(1:5, each=20))

#Package that can run multinomial logistic regression
library(VGAM)


m_model <- vglm(y ~ x, multinomial)
summary(m_model)
```

```{r}
round(fitted(m_model), 2)
```
\newpage
```{r}
(fit <- unlist(apply(round(fitted(m_model), 2), 1,
             function(x) as.numeric(which(x == max(x))[1]))))
```

```{r}
table(true=y, predict=fit)
```

# 4. Example on the Textbook
```{r}
library(ISLR)
summary(Default)
```

```{r, fig.align='center'}
with(Default, plot(balance, income, type="n", xlab="Balance", ylab="Income"))
split_data <- split(Default, Default$default)

no_df <- split_data[[1]]
df <- split_data[[2]]

with(no_df, points(balance, income, pch=1, cex=0.8, col="dodgerblue"))
with(df, points(balance, income, pch="+", cex=1.2, col="darkorange"))
```

```{r}
library(car)
Default$default <- as.numeric(as.character(
    recode(Default$default, "'Yes'='1'; 'No'='0'")))
```

```{r}
#Page 131
#Regression
model1 <- lm(default ~ balance, data=Default)

#Logistic Regression
model2 <- glm(default ~ balance, data=Default, family=binomial)
```

```{r, fig.align='center', fig.height=4, fig.width=7.5}
par(mfrow=c(1, 2))
with(Default, plot(balance, default, col="darkorange",
                   pch="|", cex=0.5, main="Regression",
                   xlab="Balance", ylab="Probability Default"))
abline(h=c(1, 0), lty=2)
abline(model1, col="dodgerblue", lwd=2)


with(Default, plot(balance, default, col="darkorange",
                   pch="|", cex=0.5, main="Logistic Regression",
                   xlab="Balance", ylab="Probability Default"))
abline(h=c(1, 0), lty=2)

curve(predict(model2, data.frame(balance=x), type="resp"),
      add=TRUE, lwd=2, col="dodgerblue")
```


```{r}
#balance
model2 <- glm(default ~ balance, data=Default, family=binomial)
#student
model3 <- glm(default ~ student, data=Default, family=binomial)
#all
model4 <- glm(default ~ balance + income + student, data=Default, family=binomial)
```

```{r, fig.align='center'}
Default$student <- as.numeric(as.character(recode(Default$student, "'Yes'='1'; 'No'='0'")))

fit_p <- fitted(model4)
D <- cbind(Default, fit_p)
D <- D[order(D$balance), ]
average <- tapply(Default$default, Default$student, mean)
plot(D$balance[D$student == 1], D$fit_p[D$student == 1],
     main="Student vs. Non Student",
     xlab="Balance", ylab="Probability",
     type="l", col="dodgerblue", lwd=2)
lines(D$balance[D$student == 0], D$fit_p[D$student == 0],
      type="l", col="darkorange", lwd=2)
abline(h=average[1], col="darkorange", lty=2, lwd=2)
abline(h=average[2], col="dodgerblue", lty=2, lwd=2)
legend(500, 0.8, lty=1, lwd=2, col=c("dodgerblue","darkorange"),
       legend=c("Student", "Nonstudent"))
```

```{r, fig.align='center'}
boxplot(balance ~ student, data=Default,
        main="Balance for student & non student",
        xlab="Student", ylab="Balance")
```

